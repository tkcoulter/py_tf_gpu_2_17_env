{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Component X\n",
    "This notebook will explore the SCANIA Component X dataset.\n",
    "\n",
    "We start by exploring the dataset. Next we will transform the data to remove any missing values, add and remvoe features. Finally a model that will predict the maintenance needs for the X-component.\n",
    "\n",
    "[Paper at arXiv](https://arxiv.org/abs/2401.15199)\n",
    "\n",
    "[Dataset can be downloaded here](https://stockholmuniversity.app.box.com/s/anmg5k93pux5p6decqzzwokp9vuzmdkh)\n",
    "\n",
    "Place the data files in the folder /data/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Includes\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import os\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/workspaces/py_tf_2_17_env\n",
      "['validation_labels.csv', 'test_specifications.csv', 'train_specifications.csv', 'train_tte.csv', 'test_operational_readouts.csv', 'train_operational_readouts.csv', 'validation_specifications.csv', 'validation_operational_readouts.csv', 'test_labels.csv']\n"
     ]
    }
   ],
   "source": [
    "print(os.getcwd())\n",
    "data_dir = \"/workspaces/datasets/scania\"\n",
    "print(os.listdir(data_dir))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'data_dir' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m#Read the raw data\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m#Train data\u001b[39;00m\n\u001b[0;32m----> 4\u001b[0m tteTrain \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(\u001b[43mdata_dir\u001b[49m,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrain_tte.csv\u001b[39m\u001b[38;5;124m'\u001b[39m))\n\u001b[1;32m      5\u001b[0m specificationsTrain \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(data_dir,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrain_specifications.csv\u001b[39m\u001b[38;5;124m'\u001b[39m))\n\u001b[1;32m      6\u001b[0m readoutsTrain \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(data_dir,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrain_operational_readouts.csv\u001b[39m\u001b[38;5;124m'\u001b[39m))\n",
      "\u001b[0;31mNameError\u001b[0m: name 'data_dir' is not defined"
     ]
    }
   ],
   "source": [
    "#Read the raw data\n",
    "#Train data\n",
    "\n",
    "tteTrain = pd.read_csv(os.path.join(data_dir,'train_tte.csv'))\n",
    "specificationsTrain = pd.read_csv(os.path.join(data_dir,'train_specifications.csv'))\n",
    "readoutsTrain = pd.read_csv(os.path.join(data_dir,'train_operational_readouts.csv'))\n",
    "\n",
    "#Validation data\n",
    "labelsValidation = pd.read_csv(os.path.join(data_dir,'validation_labels.csv'))\n",
    "specificationsValidation = pd.read_csv(os.path.join(data_dir,'validation_specifications.csv'))\n",
    "readoutsValidation = pd.read_csv(os.path.join(data_dir,'validation_operational_readouts.csv'))\n",
    "\n",
    "#Test data\n",
    "specificationsTest = pd.read_csv(os.path.join(data_dir,'test_specifications.csv'))\n",
    "readoutsTest = pd.read_csv(os.path.join(data_dir,'test_operational_readouts.csv'))\n",
    "test_labels = pd.read_csv(os.path.join(data_dir,'test_labels.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#See what is in the data\n",
    "tteTrain.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "specificationsTrain.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "readoutsTrain.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "readoutsTrain.columns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Check the shape of the data\n",
    "readoutsTrain.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "readoutsTrain.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Check how many vehicles are in the data\n",
    "readoutsTrain['vehicle_id'].unique().shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Variable structure\n",
    "\n",
    "Histogram variables use the following indexing format: variableid_binindex. Where the \"variableid\"\n",
    "represents the ID of an anonymized variable or feature, and \"binindex\" shows the bin numbers. As an example, the variable\n",
    "with \"variableid\" 167 is a multi-dimensional histogram that has ten bins, \"167_0\", \"167_1\",..., and \"167_9\".\n",
    "\n",
    "In summary, six out of 14 variables are organized into six histograms with variable IDs: \"167\", \"272\", \"291\", \"158\", \"459\",\n",
    "and \"397,\" with 10, 10, 11, 10, 20, and 36 bins, respectively.\n",
    "\n",
    "Moreover, the eight rest of the variables\n",
    "named \"171_0\", \"666_0\", \"427_0\", \"837_0\", \"309_0\", \"835_0\", \"370_0\", \"100_0\" are numerical counters. These features are\n",
    "accumulative and are suitable for the representation of trends over time.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = ['167_0', '167_1', '167_2', '167_3', '167_4', '167_5', '167_6', '167_7', '167_8', '167_9']\n",
    "#Take the first row of the data\n",
    "y = readoutsTrain[cols].iloc[0]\n",
    "#Plot the data\n",
    "plt.plot(y, 'o-')\n",
    "plt.xlabel('Bin')\n",
    "plt.ylabel('Value')\n",
    "plt.title('Sample readout for variable 167, as a histogram of ten bins');\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = ['459_0', '459_1', '459_2', '459_3', '459_4', '459_5', '459_6', '459_7', '459_8', '459_9', '459_10', '459_11', '459_12', '459_13', '459_14', '459_15', '459_16', '459_17', '459_18', '459_19']\n",
    "#Take the first row of the data\n",
    "y = readoutsTrain[cols].iloc[0]\n",
    "#Plot the data\n",
    "plt.figure(figsize = (10,10))\n",
    "plt.plot(y, 'o-')\n",
    "plt.xlabel('Bin')\n",
    "plt.ylabel('Value')\n",
    "plt.title('Sample readout for variable 459, as a histogram of twenty bins');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols =  ['171_0', '666_0', '427_0', '837_0', '309_0', '835_0', '370_0', '100_0']\n",
    "vechicleId = 2\n",
    "x = readoutsTrain[readoutsTrain['vehicle_id'] == vechicleId]['time_step'].to_numpy()\n",
    "y = readoutsTrain[readoutsTrain['vehicle_id'] == vechicleId][cols].to_numpy()\n",
    "plt.figure(figsize = (10,10))\n",
    "plt.plot(x, y)\n",
    "plt.xlabel('Time')\n",
    "plt.ylabel('Value')\n",
    "plt.legend(cols)\n",
    "plt.title('Sample readout for vehicle 2');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Correlation of non-histogram features in the training set\n",
    "See what the correlations of the non-histogram features are, for the entire training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols =  ['171_0', '666_0', '427_0', '837_0', '309_0', '835_0', '370_0', '100_0']\n",
    "\n",
    "#Calculate the correlation matrix\n",
    "corr = readoutsTrain[cols].corr()\n",
    "\n",
    "#Plot the correlation matrix\n",
    "plt.figure(figsize = (10,10))\n",
    "ax = sns.heatmap(corr, annot=True, fmt=\".2f\")\n",
    "\n",
    "#Add title\n",
    "plt.title('Correlation matrix of the non-histogram variables');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the number of missing values per column\n",
    "y = readoutsTrain.isnull().sum()\n",
    "plt.figure(figsize = (10,10))\n",
    "plt.bar(y.index, y)\n",
    "plt.xlabel('Number of missing values')\n",
    "plt.ylabel('Variable')\n",
    "plt.title('Number of missing values per variable in the readouts data');\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualize the file train_tte.csv\n",
    "\n",
    "The file with the name \"train_tte.csv\" contains the repair records of Component X collected from each vehicle, indicating\n",
    "the time_to_event (tte), i.e., the replacement time for Component X during the study period. This data file includes 23550\n",
    "number of rows and two columns: \"length_of_study_time_step\" and \"in_study_repair,\" where the former indicates the number\n",
    "of operation time steps after Component X started working. The latter is the class label, where itâ€™s set to 1 if Component X was\n",
    "repaired at the time equal to its corresponding length_of_study_time_step, or it can take the value of zero in case no failure or\n",
    "repair event occurs during the first length_of_study_time_step of operation. It is good to mention that the \"train_tte.csv\" data\n",
    "is imbalanced with 21278 occurrences of label 0 and 2272 instances of label 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tteTrain.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Check the number of data points in the Time To Event data\n",
    "tteTrain.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tteTrain.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Check if there are nulls in the data\n",
    "tteTrain.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Count the number of unique vehicles\n",
    "tteTrain['vehicle_id'].unique().shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Check the distribution of events, if the vechicle is 1, then a failure happened, and 0 no failure happened\n",
    "tteTrain['in_study_repair'].hist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(tteTrain['length_of_study_time_step'], bins = 100);\n",
    "plt.xlabel('Length of study time step')\n",
    "plt.ylabel('Frequency')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The file train_specifications.csv\n",
    "\n",
    "The last file in the training set is called \"train_specifications.csv,\" which contains information about the specifications of the\n",
    "vehicles, such as their engine type and wheel configuration. In total, there are 23550 observations and eight categorical features\n",
    "for all vehicles. The features in train_specifications.csv are anonymized, each can take categories in Cat0, Cat1, ..., Cat8. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "specificationsTrain.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For each column create a subplot with the histogram of the data\n",
    "plt.figure(figsize = (20,20))\n",
    "for i, col in enumerate(specificationsTrain.columns[1:]):\n",
    "    plt.subplot(4, 4, i+1)\n",
    "    plt.hist(specificationsTrain[col])\n",
    "    plt.title(col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Check for nans\n",
    "specificationsTrain.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tteTrain[tteTrain['in_study_repair'] == 1].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tteTrain.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols =  ['171_0', '666_0', '427_0', '837_0', '309_0', '835_0', '370_0', '100_0']\n",
    "vechicleId = 22\n",
    "x = readoutsTrain[readoutsTrain['vehicle_id'] == vechicleId]['time_step'].to_numpy()\n",
    "y = readoutsTrain[readoutsTrain['vehicle_id'] == vechicleId][cols].to_numpy()\n",
    "plt.figure(figsize = (10,10))\n",
    "plt.plot(x, y)\n",
    "plt.xlabel('Time')\n",
    "plt.ylabel('Value')\n",
    "plt.legend(cols)\n",
    "plt.title(f'Sample readout for vehicle {vechicleId}');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "readoutsTrain[readoutsTrain['vehicle_id'] == vechicleId]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Format of validation labels\n",
    "The validation_labels.csv file has 5046 rows, which is equal to the number of vehicles contributed to the operational data of the\n",
    "validation set. It includes a column named class_label, corresponding to the class for the last readout of each vehicle.\n",
    "\n",
    "The temporal placement of this final simulated readout is categorized into five classes\n",
    "denoted by 0, 1, 2, 3, 4 where they are related to readouts within a time window of: (more than 48), (48 to 24), (24 to 12), (12\n",
    "to 6), and (6 to 0) time_step before the failure, respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labelsValidation.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labelsValidation['class_label'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(labelsValidation['class_label'])\n",
    "plt.xlabel('Class label')\n",
    "plt.ylabel('Count')\n",
    "plt.title('Distribution of class labels in the validation data');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Remove NaNs\n",
    "We need to remove missing values and see if there are any strange values in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#First we group by vehicle_id and we will forward fill the last known value.\n",
    "#Then if the entire column is NaN, we will fill it with the median of the column. \n",
    "#If there are still any NaNs we will fill them with 0.\n",
    "\n",
    "def fill_missing_values(df):\n",
    "    df = df.groupby('vehicle_id').apply(lambda x: x.ffill(axis=0)) #Forward fill last known value, but only for the same vehicle\n",
    "    df = df.droplevel('vehicle_id') #Remove multi-index, as we don't want to group by vehicle_id anymore    \n",
    "    df = df.fillna(df.median()) #Fill with median rather than mean to avoid outliers\n",
    "    df = df.fillna(0) #Last resort fill with 0\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Clean the data\n",
    "print('Cleaning the training data')\n",
    "print(f'Number of missing values before cleaning: {readoutsTrain.isnull().sum().sum()}')\n",
    "readoutsTrain = fill_missing_values(readoutsTrain)\n",
    "print(f'Number of missing values after cleaning: {readoutsTrain.isnull().sum().sum()}')\n",
    "#readoutsValidation = fill_missing_values(readoutsValidation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "readoutsTrain.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Class labels\n",
    "Create the class labels for our training set. There are multiple ways to do this. Lets start by denoting the labels as they are in the validation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We want to create labels for the training data based on the time to event data\n",
    "# Labels in validation set are denoted by 0, 1, 2, 3, 4 where they are related to readouts within a time window of: (more than 48), (48 to 24), (24 to 12), (12 to 6), and (6 to 0) time_step before the failure, respectively. \n",
    "# If we don't have a failure reported, and the time_step left is less 48 we don't know when the failure will happen, so we will label it as -1. \n",
    "\n",
    "def get_class_label(row):\n",
    "    #classes denoted by 0, 1, 2, 3, 4 where they are related to readouts within a time window of: (more than 48), (48 to 24), (24 to 12), (12 to 6), and (6 to 0) time_step before the failure, respectively\n",
    "    if row['time_to_potential_event'] > 48:\n",
    "        return 0 #No failure within 48 time steps\n",
    "    elif row['time_to_potential_event'] > 24 and row['in_study_repair'] == 1:\n",
    "        return 1 #Failure within 48 to 24 time steps\n",
    "    elif row['time_to_potential_event'] > 12 and row['in_study_repair'] == 1:\n",
    "        return 2 #Failure within 24 to 12 time steps\n",
    "    elif row['time_to_potential_event'] > 6 and row['in_study_repair'] == 1:\n",
    "        return 3 #Failure within 12 to 6 time steps\n",
    "    elif row['time_to_potential_event'] > 0 and row['in_study_repair'] == 1:\n",
    "        return 4 #Failure within 6 to 0 time steps\n",
    "    else:\n",
    "        return -1 #No failure reported, but within 48 time steps from the end of the study, don't know if it will fail or not\n",
    "    \n",
    "def add_class_labels(tte, readouts):\n",
    "    # Join the readouts and the time to event data\n",
    "    df = pd.merge(readouts, tteTrain, on = 'vehicle_id', how='left').copy()\n",
    "\n",
    "    #Calculate the time to a failure event\n",
    "    df['time_to_potential_event'] = df['length_of_study_time_step'] - df['time_step']\n",
    "\n",
    "    df['class_label'] = df.apply(get_class_label, axis=1)\n",
    "\n",
    "    return df\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Merge the time to event data with the readouts data and figure out which class they belong to\n",
    "#Later we will need to remove the columns: length_of_study_time_step, in_study_repair, time_to_potential_event, class_label and remove any rows with class label -1\n",
    "dfTrain = add_class_labels(tteTrain, readoutsTrain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfTrain.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfTrain.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature engineering\n",
    "Lets create features that can be used in a ML model."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0rc1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
